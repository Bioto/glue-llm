# GlueLLM Configuration
# Copy this file to .env and configure your settings

# Default model to use (format: "provider:model_name")
GLUELLM_DEFAULT_MODEL=openai:gpt-4o-mini

# Default system prompt for all completions
GLUELLM_DEFAULT_SYSTEM_PROMPT=You are a helpful assistant.

# Maximum number of tool execution iterations (prevents infinite loops)
GLUELLM_MAX_TOOL_ITERATIONS=10

# Retry configuration
GLUELLM_RETRY_MAX_ATTEMPTS=3
GLUELLM_RETRY_MIN_WAIT=2
GLUELLM_RETRY_MAX_WAIT=30
GLUELLM_RETRY_MULTIPLIER=1

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
GLUELLM_LOG_LEVEL=INFO

# Optional: API keys (can also be set via provider-specific env vars)
# GLUELLM_OPENAI_API_KEY=your-openai-api-key
# GLUELLM_ANTHROPIC_API_KEY=your-anthropic-api-key
# GLUELLM_XAI_API_KEY=your-xai-api-key

# Provider-specific API keys (recognized by any-llm-sdk)
# OPENAI_API_KEY=your-openai-api-key
# ANTHROPIC_API_KEY=your-anthropic-api-key
# XAI_API_KEY=your-xai-api-key

# OpenTelemetry Tracing Configuration
# Enable distributed tracing with MLflow for observability
# GLUELLM_ENABLE_TRACING=false
# GLUELLM_MLFLOW_TRACKING_URI=http://localhost:5000
# GLUELLM_MLFLOW_EXPERIMENT_NAME=gluellm
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5000/v1/traces
